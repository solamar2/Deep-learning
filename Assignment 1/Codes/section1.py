
import numpy as np
import math as math
from section1 import *
from section3 import *

# Implement the following functions, which are used to carry out the forward propagation process

def initialize_parameters(layer_dims):
    """
    Input: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened input, layer L is the output softmax)
    Output: a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL).
    """
    # defining dictionary for the parameters of the diffrent layers
    P_dict={}
    # Loop that going over the input in order to initialize each layers parameters

    for lay_dim in range(1,len(layer_dims)):

      P_dict['W'+str(lay_dim)]=np.random.uniform(-1/np.sqrt(layer_dims[lay_dim]),1/np.sqrt(layer_dims[lay_dim]),(layer_dims[lay_dim],layer_dims[lay_dim-1]))
      P_dict['b'+str(lay_dim)]=np.zeros((layer_dims[lay_dim],1))

    return P_dict

# Implement the linear part of a layer's forward propagation

"""###############################################"""

def	linear_forward(A, W, b):
  """
Input:
A – the activations of the previous layer
W – the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
B – the bias vector of the current layer (of shape [size of current layer, 1])

Output:
Z – the linear component of the activation function (i.e., the value before applying the non-linear function)
linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute)
  """
  linear_cache={}
  linear_cache = {'A': A, 'W': W, 'b': b}
  Z=np.dot(W,A)+b
  return Z,linear_cache

"""###############################################"""

def softmax(Z):
  """
Input:
Z – the linear component of the activation function

Output:
A – the activations of the layer
activation_cache – returns Z, which will be useful for the backpropagation
  """
  activation_cache=Z
  A=np.exp(Z)/np.sum(np.exp(Z),axis=0)
  

  return A,activation_cache

"""###############################################"""

def	relu(Z):
  """
Input:
Z – the linear component of the activation function

Output:
A – the activations of the layer
activation_cache – returns Z, which will be useful for the backpropagation
"""
  activation_cache=Z
  A=np.maximum(Z,0)
  

  return A,activation_cache

"""###############################################"""

def	apply_batchnorm(A):
  """
  Description:
performs batchnorm on the received activation values of a given layer.

Input:
A - the activation values of a given layer

output:
NA - the normalized activation values, based on the formula learned in class
"""
  epsilon=0.001
  NA=(A-np.mean(A))/math.sqrt((np.var(A)+epsilon))
  return NA

"""###############################################"""

def	linear_activation_forward(A_prev, W, B, activation):
  """
Description:
Implement the forward propagation for the LINEAR->ACTIVATION layer

Input:
A_prev – activations of the previous layer
W – the weights matrix of the current layer
B – the bias vector of the current layer
Activation – the activation function to be used (a string, either “softmax” or “relu”)

Output:
A – the activations of the current layer
cache – a joint dictionary containing both linear_cache and activation_cache

"""
  Z,linear_cache= linear_forward(A_prev, W, B)

  if activation == 'relu':
        A,activation_cache = relu(Z)
  elif activation == 'softmax':
        A,activation_cache = softmax(Z)
  else:
        print(" activation function: {} isn't good, choose relu or softmax".format(activation))
        return

  cache = {'linear_cache': linear_cache, 'activation_cache': activation_cache}

  return A,cache

"""###############################################"""

def	L_model_forward(X, parameters, use_batchnorm):
  """
Description:
Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation

Input:
X – the data, numpy array of shape (input size, number of examples)
parameters – the initialized W and b parameters of each layer
use_batchnorm - a boolean flag used to determine whether to apply batchnorm after the activation (note that this option needs to be set to “false” in Section 3 and “true” in Section 4).

Output:
AL – the last post-activation value
caches – a list of all the cache objects generated by the linear_forward function
  """
  L=len(parameters)/2
  L=int(L)
  A=X
  caches=[]
  for i in range(1,L):
    A,cache=linear_activation_forward(A, parameters['W'+str(i)], parameters['b'+str(i)],'relu')
    caches.append(cache)
    if use_batchnorm=='True':
      A=apply_batchnorm(A)

  AL,cache=linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)],'softmax')
  caches.append(cache)

  return AL,caches

"""###############################################"""

def	compute_cost(AL, Y):
  """
Input:
AL – probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
Y – the labels vector (i.e. the ground truth)

Output:
cost – the cross-entropy cost
  """

  """
  num_samples=Y.shape[1]
  cost=-1/(num_samples)*np.sum(np.sum((Y*np.log(AL)+(1-Y)*np.log(1-AL)),axis=0))
  """
  m = AL.shape[1] # number of examples
  cost = (-1/m) * np.sum(np.sum(Y * np.log(AL), axis=1))
  return cost
