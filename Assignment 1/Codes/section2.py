# Implement the following functions, which are used to carry out the backward propagation process:
import numpy as np
import math as math
from section1 import *
from section2 import *


def Linear_backward(dZ, cache):
  """
description:
Implements the linear part of the backward propagation process for a single layer

Input:
dZ – the gradient of the cost with respect to the linear output of the current layer (layer l)
cache – tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

Output:
dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev, dL/dZ*dZ/dx=dZ*W
dW -- Gradient of the cost with respect to W (current layer l), same shape as W
db -- Gradient of the cost with respect to b (current layer l), same shape as b
"""
  dA_prev=np.dot(cache['W'].T,dZ)
  m=dZ.shape[1]
  dW=(1/m)*np.dot(dZ,cache['A'].T)
  db=(1/m)*np.sum(dZ,axis=1,keepdims=True)
  return dA_prev,dW,db

"""########################################################################"""

def	relu_backward (dA, activation_cache):
  """Description:
Implements backward propagation for a ReLU unit
dZ=dL/dz=(dL/dA)*(dA/dZ)=dA*relu(z)'
Input:
dA – the post-activation gradient
activation_cache – contains Z (stored during the forward propagation)

Output:
dZ – gradient of the cost with respect to Z
  """
  relu_tag =  (activation_cache > 0).astype(float)
  dZ=relu_tag*dA
  return dZ

"""############################################################################"""

def softmax_backward(dA, activation_cache):
  """
  Description:
Implements backward propagation for a softmax unit

Input:
dA – the post-activation gradient
activation_cache – contains Z (stored during the forward propagation)

Output:
dZ – gradient of the cost with respect to Z
  """
  A=np.exp(activation_cache)/np.sum(np.exp(activation_cache),axis=0)
  softmax_tag=A*(1-A)
  dZ=(softmax_tag*dA)
  return dZ

"""########################################################################"""

def	linear_activation_backward(dA, cache, activation):
  """
  Description:
Implements the backward propagation for the LINEAR->ACTIVATION layer. The function first computes dZ and then applies the linear_backward function.

Input:
dA – post activation gradient of the current layer
cache – contains both the linear cache and the activations cache

Output:
dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
dW – Gradient of the cost with respect to W (current layer l), same shape as W
db – Gradient of the cost with respect to b (current layer l), same shape as b
  """
  activation_cache=cache['activation_cache']
  if activation == 'relu':
        dZ = relu_backward(dA, activation_cache)
  elif activation == 'softmax':
        dZ = softmax_backward(dA, activation_cache)
  else:
        print(" activation function: {} isn't good, choose relu or softmax".format(activation))
        return

  dA_prev,dW,db=Linear_backward(dZ, cache['linear_cache'])

  return dA_prev,dW,db

"""########################################################################"""
def	L_model_backward(AL, Y, caches):
  """
  Description:
 Implement the backward propagation process for the entire network.
Some comments:
the backpropagation for the softmax function should be done only once as only the output layers uses it and the RELU should be done iteratively over all the remaining layers of the network.
Input:
AL - the probabilities vector, the output of the forward propagation (L_model_forward)
Y - the true labels vector (the "ground truth" - true classifications)
Caches - list of caches containing for each layer: a) the linear cache; b) the activation cache

Output:
Grads - a dictionary with the gradients
             grads["dA" + str(l)] = ...
             grads["dW" + str(l)] = ...
             grads["db" + str(l)] = ...
  """

  L=len(caches)
  L=int(L)
  grads={}
  dA=-np.divide(Y,AL+1e-10)+np.divide((1-Y),(1-AL+1e-10))
  #the last layer which is softmax in our case
  grads["dA" + str(L)]=dA
  dA,dW,db=linear_activation_backward(dA, caches[L-1], 'softmax')
  grads["dW" + str(L)] =dW
  grads["db" + str(L)] = db

  lay_N = list(range(0, L-1))
  lay_N.reverse()
  
  for i in lay_N:
    
    grads["dA" + str(i+1)] = dA 
    dA,dW,db=linear_activation_backward(dA, caches[i], 'relu')
    grads["dW" + str(i+1)] =dW
    grads["db" + str(i+1)] = db

  return grads


"""########################################################################"""
def	Update_parameters(parameters, grads, learning_rate):
  """Description:
Updates parameters using gradient descent

Input:
parameters – a python dictionary containing the DNN architecture’s parameters
grads – a python dictionary containing the gradients (generated by L_model_backward)
learning_rate – the learning rate used to update the parameters (the “alpha”)

Output:
parameters – the updated values of the parameters object provided as input
  """
  L=len(parameters)/2
  L=int(L)
  for i in range(1,L+1):
    parameters['W'+str(i)]=parameters['W'+str(i)]-learning_rate*grads["dW" + str(i)]
    parameters['b'+str(i)]=parameters['b'+str(i)]-learning_rate*grads["db" + str(i)][:][1]

  return parameters